# -*- coding: utf-8 -*-
"""project 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DmN_wrBSfD36NUP9iabtOeS1nP1VNBDv
"""

!pip install -U transformers

"""## Local Inference on GPU
Model page: https://huggingface.co/ibm-granite/granite-3.0-2b-instruct

‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/ibm-granite/granite-3.0-2b-instruct)
			and/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè
"""

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("text-generation", model="ibm-granite/granite-3.0-2b-instruct")
messages = [
    {"role": "user", "content": "where are you?"},
]
pipe(messages)

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import os

# Auto-detect device and configure dtype
device = "cuda" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 if device == "cuda" else torch.float32

# Load model and tokenizer
model_name = "ibm-granite/granite-3.0-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto" if device == "cuda" else None,
    torch_dtype=torch_dtype
)

if device == "cpu":
    model = model.to(device)

# System prompt for finance guidance
SYSTEM_PROMPT = """You are a knowledgeable and helpful AI Finance Assistant. Your role is to provide clear, accurate, and practical financial guidance to users. You can help with budgeting, investment basics, tax planning, emergency funds, savings strategies, debt management, and general financial literacy. Always provide educational information and encourage users to consult with licensed financial advisors for personalized advice. Be supportive, clear, and avoid financial jargon when possible."""

# Format chat history into role-based messages
def format_messages(history, system_prompt):
    messages = [{"role": "system", "content": system_prompt}]
    for user_msg, assistant_msg in history:
        if user_msg:
            messages.append({"role": "user", "content": user_msg})
        if assistant_msg:
            messages.append({"role": "assistant", "content": assistant_msg})
    return messages

# Generate response
def generate_response(message, history, temperature, top_p):
    messages = format_messages(history, SYSTEM_PROMPT)
    messages.append({"role": "user", "content": message})

    # Apply chat template
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    # Decode response
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extract assistant response
    if "<|assistant|>" in full_response:
        response = full_response.split("<|assistant|>")[-1].strip()
    else:
        response = full_response[len(prompt):].strip()

    return response

# Chat function
def chat(message, history, temperature, top_p):
    response = generate_response(message, history, temperature, top_p)
    history.append((message, response))
    return "", history

# Quick topic buttons
def quick_topic(topic, history, temperature, top_p):
    prompts = {
        "Budgeting": "Can you help me understand how to create a personal budget?",
        "Investment Basics": "What are the basics of investing I should know as a beginner?",
        "Tax Planning": "What are some key tax planning strategies I should consider?",
        "Emergency Fund": "How do I build an emergency fund and how much should I save?"
    }
    message = prompts.get(topic, topic)
    response = generate_response(message, history, temperature, top_p)
    history.append((message, response))
    return history

# Custom CSS
custom_css = """
#main-container {
    max-width: 1400px;
    margin: 0 auto;
}
#chatbot {
    height: 600px;
    border-radius: 10px;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
}
.gradio-container {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}
#title {
    text-align: center;
    color: #2c3e50;
    font-size: 2.5em;
    font-weight: bold;
    margin-bottom: 10px;
}
#subtitle {
    text-align: center;
    color: #7f8c8d;
    font-size: 1.1em;
    margin-bottom: 20px;
}
#disclaimer {
    background-color: #fff3cd;
    border: 1px solid #ffc107;
    border-radius: 5px;
    padding: 15px;
    margin-top: 20px;
    color: #856404;
    font-size: 0.9em;
}
.quick-btn {
    margin: 5px;
    font-weight: 500;
}
#sidebar {
    background-color: #f8f9fa;
    padding: 20px;
    border-radius: 10px;
}
"""

# Build Gradio interface
with gr.Blocks(css=custom_css, theme=gr.themes.Soft()) as app:
    gr.HTML("<div id='title'>üí∞ AI Finance Assistant</div>")
    gr.HTML("<div id='subtitle'>Your intelligent companion for financial guidance powered by IBM Granite</div>")

    with gr.Row(elem_id="main-container"):
        with gr.Column(scale=3):
            chatbot = gr.Chatbot(
                elem_id="chatbot",
                label="Chat",
                show_label=False,
                avatar_images=(None, "https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png")
            )

            with gr.Row():
                msg = gr.Textbox(
                    placeholder="Ask me anything about finance...",
                    show_label=False,
                    scale=4
                )
                send = gr.Button("Send", variant="primary", scale=1)

            with gr.Row():
                gr.Markdown("### Quick Topics")
            with gr.Row():
                btn_budget = gr.Button("üíµ Budgeting", elem_classes="quick-btn", size="sm")
                btn_invest = gr.Button("üìà Investment Basics", elem_classes="quick-btn", size="sm")
                btn_tax = gr.Button("üìã Tax Planning", elem_classes="quick-btn", size="sm")
                btn_emergency = gr.Button("üÜò Emergency Fund", elem_classes="quick-btn", size="sm")

        with gr.Column(scale=1, elem_id="sidebar"):
            gr.Markdown("### ‚öôÔ∏è Settings")
            temperature = gr.Slider(
                minimum=0.1,
                maximum=1.0,
                value=0.7,
                step=0.1,
                label="Temperature",
                info="Controls randomness"
            )
            top_p = gr.Slider(
                minimum=0.1,
                maximum=1.0,
                value=0.9,
                step=0.05,
                label="Top P",
                info="Nucleus sampling"
            )
            clear = gr.Button("üóëÔ∏è Clear Chat", variant="secondary")

            gr.Markdown("### ‚ÑπÔ∏è Model Info")
            gr.Markdown(f"""
            **Model:** IBM Granite 3.0 2B
            **Device:** {device.upper()}
            **Dtype:** {torch_dtype}
            """
            )

    gr.HTML("""
    <div id='disclaimer'>
        <strong>‚ö†Ô∏è Disclaimer:</strong> This AI assistant provides general financial education and information only.
        It is not a substitute for professional financial advice. Always consult with qualified financial advisors,
        tax professionals, or legal experts before making financial decisions. Past performance does not guarantee future results.
    </div>
    """
    )

    # Event handlers
    msg.submit(chat, [msg, chatbot, temperature, top_p], [msg, chatbot])
    send.click(chat, [msg, chatbot, temperature, top_p], [msg, chatbot])
    clear.click(lambda: [], None, chatbot)

    btn_budget.click(
        lambda h, t, p: quick_topic("Budgeting", h, t, p),
        [chatbot, temperature, top_p],
        chatbot
    )
    btn_invest.click(
        lambda h, t, p: quick_topic("Investment Basics", h, t, p),
        [chatbot, temperature, top_p],
        chatbot
    )
    btn_tax.click(
        lambda h, t, p: quick_topic("Tax Planning", h, t, p),
        [chatbot, temperature, top_p],
        chatbot
    )
    btn_emergency.click(
        lambda h, t, p: quick_topic("Emergency Fund", h, t, p),
        [chatbot, temperature, top_p],
        chatbot
    )

# Launch app
if __name__ == "__main__":
    app.launch(server_name="0.0.0.0", server_port=7861, share=True)

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("ibm-granite/granite-3.0-2b-instruct")
model = AutoModelForCausalLM.from_pretrained("ibm-granite/granite-3.0-2b-instruct")
messages = [
    {"role": "user", "content": "Who are you?"},
]
inputs = tokenizer.apply_chat_template(
	messages,
	add_generation_prompt=True,
	tokenize=True,
	return_dict=True,
	return_tensors="pt",
).to(model.device)

outputs = model.generate(**inputs, max_new_tokens=40)
print(tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:]))

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import os

device = "cuda" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 if device == "cuda" else torch.float32

model_name = "ibm-granite/granite-3.0-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto" if device == "cuda" else None,
    torch_dtype=torch_dtype
)

if device == "cpu":
    model = model.to(device)

SYSTEM_PROMPT = """You are a helpful and compassionate AI assistant for CareWell Hospital. Your role is to provide clear information about hospital services, departments, visiting hours, appointments, facilities, and general health guidance. You assist patients, visitors, and their families with navigating hospital services and understanding general medical information. Always be professional, empathetic, and supportive. Provide educational information only - do not diagnose conditions, prescribe treatments, or replace licensed healthcare professionals. For urgent medical situations, always advise seeking immediate medical attention. Encourage users to consult with qualified healthcare providers for personal medical concerns."""

def format_messages(history, system_prompt):
    messages = [{"role": "system", "content": system_prompt}]
    for user_msg, assistant_msg in history:
        if user_msg:
            messages.append({"role": "user", "content": user_msg})
        if assistant_msg:
            messages.append({"role": "assistant", "content": assistant_msg})
    return messages

def generate_response(message, history, temperature, top_p):
    messages = format_messages(history, SYSTEM_PROMPT)
    messages.append({"role": "user", "content": message})

    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    if "<|assistant|>" in full_response:
        response = full_response.split("<|assistant|>")[-1].strip()
    else:
        response = full_response[len(prompt):].strip()

    return response

def chat(message, history, temperature, top_p):
    response = generate_response(message, history, temperature, top_p)
    history.append((message, response))
    return "", history

def quick_topic(topic, history, temperature, top_p):
    prompts = {
        "Visiting Hours": "What are the visiting hours at CareWell Hospital for different departments?",
        "Facilities": "Can you tell me about the facilities and services available at CareWell Hospital?",
        "Contact Number": "What are the contact numbers for CareWell Hospital including emergency, appointments, and general inquiries?"
    }
    message = prompts.get(topic, topic)
    response = generate_response(message, history, temperature, top_p)
    history.append((message, response))
    return history

custom_css = """
#main-container {
    max-width: 1400px;
    margin: 0 auto;
}
#chatbot {
    height: 600px;
    border-radius: 10px;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
}
.gradio-container {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}
#title {
    text-align: center;
    color: #2c5aa0;
    font-size: 2.5em;
    font-weight: bold;
    margin-bottom: 10px;
}
#subtitle {
    text-align: center;
    color: #5a7ba6;
    font-size: 1.1em;
    margin-bottom: 20px;
}
#disclaimer {
    background-color: #fff3cd;
    border: 1px solid #ffc107;
    border-radius: 5px;
    padding: 15px;
    margin-top: 20px;
    color: #856404;
    font-size: 0.9em;
}
.quick-btn {
    margin: 5px;
    font-weight: 500;
}
#sidebar {
    background-color: #f8f9fa;
    padding: 20px;
    border-radius: 10px;
}
"""

with gr.Blocks(css=custom_css, theme=gr.themes.Soft()) as app:
    gr.HTML("<div id='title'>üè• CareWell Hospital AI Assistant</div>")
    gr.HTML("<div id='subtitle'>Your intelligent companion for hospital information and health guidance powered by IBM Granite</div>")

    with gr.Row(elem_id="main-container"):
        with gr.Column(scale=3):
            chatbot = gr.Chatbot(
                elem_id="chatbot",
                label="Chat",
                show_label=False,
                avatar_images=(None, "https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png")
            )

            with gr.Row():
                msg = gr.Textbox(
                    placeholder="Ask me about hospital services, visiting hours, appointments, or health information...",
                    show_label=False,
                    scale=4
                )
                send = gr.Button("Send", variant="primary", scale=1)

            with gr.Row():
                gr.Markdown("### Quick Topics")
            with gr.Row():
                btn_hours = gr.Button("‚è∞ Visiting Hours", elem_classes="quick-btn", size="sm")
                btn_facilities = gr.Button("üè• Facilities", elem_classes="quick-btn", size="sm")
                btn_contact = gr.Button("üìû Contact Number", elem_classes="quick-btn", size="sm")

        with gr.Column(scale=1, elem_id="sidebar"):
            gr.Markdown("### ‚öôÔ∏è Settings")
            temperature = gr.Slider(
                minimum=0.1,
                maximum=1.0,
                value=0.7,
                step=0.1,
                label="Temperature",
                info="Controls randomness"
            )
            top_p = gr.Slider(
                minimum=0.1,
                maximum=1.0,
                value=0.9,
                step=0.05,
                label="Top P",
                info="Nucleus sampling"
            )
            clear = gr.Button("üóëÔ∏è Clear Chat", variant="secondary")

            gr.Markdown("### ‚ÑπÔ∏è Hospital Info")
            gr.Markdown("""
            **Emergency:** 911
            **Main Line:** (555) 123-4567
            **Appointments:** (555) 123-4568
            **Location:** 123 Healthcare Ave
            Medical City, MC 12345
            """)

            gr.Markdown("### ü§ñ Model Info")
            gr.Markdown(f"""
            **Model:** IBM Granite 3.0 2B
            **Device:** {device.upper()}
            **Dtype:** {torch_dtype}
            """)

    gr.HTML("""
    <div id='disclaimer'>
        <strong>‚ö†Ô∏è Medical Disclaimer:</strong> This AI assistant provides general hospital information and educational health content only.
        It is not a substitute for professional medical advice, diagnosis, or treatment. Always seek the advice of qualified healthcare
        providers with questions regarding medical conditions. For medical emergencies, call 911 immediately. This information is for
        educational purposes only and is not a substitute for professional medical advice.
    </div>
    """)

    msg.submit(chat, [msg, chatbot, temperature, top_p], [msg, chatbot])
    send.click(chat, [msg, chatbot, temperature, top_p], [msg, chatbot])
    clear.click(lambda: [], None, chatbot)

    btn_hours.click(
        lambda h, t, p: quick_topic("Visiting Hours", h, t, p),
        [chatbot, temperature, top_p],
        chatbot
    )
    btn_facilities.click(
        lambda h, t, p: quick_topic("Facilities", h, t, p),
        [chatbot, temperature, top_p],
        chatbot
    )
    btn_contact.click(
        lambda h, t, p: quick_topic("Contact Number", h, t, p),
        [chatbot, temperature, top_p],
        chatbot
    )

if __name__ == "__main__":
    app.launch(server_name="0.0.0.0", server_port=7860, share=True)